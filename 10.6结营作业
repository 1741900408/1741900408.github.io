- [4] *大语言模型*
还有开源、闭源大模型，Prompt工程，Embedding辅助给LLM外接大脑参数高效微调等等等等…………（此处省略无数个理论知识）一系列知识.

>1.使用马尔可夫假设来建模语言序列的n元语言模型
>2.基于神经网络构建语言模型，如循环神经网络，并学习上下文相关的词表示，也被称为词嵌入（Word Embedding）。代表工作：word2vec
>3.使用大量的无标注数据预训练双向LSTM或者Transformer,然后在下游任务上进行微调（Fine-Tuning）。代表性工作：ELMo，BERT，GPT-1/2
>4.基于“扩展法则”（Scaling Law），即通过增加模型训练参数或训练数据，可以提升下游任务的性能，同属具有小模型不具有的“涌现能力”（Emergent Abilities）。代表工作：GPT-3，ChatGPT,Claude,Liama


此外我学习了如何在CPU上部署大模型
